---
title: "Regulating Modality Utilization within Multimodal Fusion Networks"
collection: MDPI
permalink: /publication/2024_RegulatingMU
excerpt: 'This paper proposes a modality utilization-based training method for multimodal fusion networks, ensuring balanced integration of diverse data in aerial imagery tasks while improving noise robustness and enhancing performance in noisy conditions.'
date: 2024-09-19
venue: 'Sensors 2024, 24(18), 6054, Special Issue: Deep Learning Methods for Aerial Imagery'
paperurl: 'http://sauravsingh1245.github.io/files/2024_RegulatingMU.pdf'
citation: 'S. Singh, E. Saber, P. P. Markopoulos, and J. Heard, “Regulating Modality Utilization within Multimodal Fusion Networks,” Sensors, vol. 24, no. 18, p. 6054, 2024.'
---
Multimodal fusion networks play a pivotal role in leveraging diverse sources of information for enhanced machine learning applications in aerial imagery. However, current approaches often suffer from a bias towards certain modalities, diminishing the potential benefits of multimodal data. This paper addresses this issue by proposing a novel modality utilization-based training method for multimodal fusion networks. The method aims to guide the network's utilization on its input modalities, ensuring a balanced integration of complementary information streams, effectively mitigating the overutilization of dominant modalities. The method is validated on multimodal aerial imagery classification and image segmentation tasks, effectively maintaining modality utilization within +/-10% of the user-defined target utilization and demonstrating the versatility and efficacy of the proposed method across various applications. Furthermore, the study explores the robustness of the fusion networks against noise in input modalities, a crucial aspect in real-world scenarios. The method showcases better noise robustness by maintaining performance amidst environmental changes affecting different aerial imagery sensing modalities. The network trained with 75.0% EO utilization achieves significantly better accuracy (81.4%) in noisy conditions (noise variance = 0.12) compared to traditional training methods with 99.59% EO utilization (73.7%). Additionally, it maintains an average accuracy of 85.0% across different noise levels, outperforming the traditional method's average accuracy of 81.9%. Overall, the proposed approach presents a significant step towards harnessing the full potential of multimodal data fusion in diverse machine learning applications such as robotics, healthcare, satellite imagery, and defense applications.
![Modality utilization-based training targets the decision layers while using pre-trained feature extractors with frozen weights.](\../images/RegulatingMU.jpg)